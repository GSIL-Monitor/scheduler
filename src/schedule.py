"""
Job schuduler class

1, run the jobs accroding to the time constains, both at-time and periodic
2, run the jobs accroding to the dependency status
3, control the job running parallelly, and each job runs within a seperate thread
4, privode a daemon thread as the central scheduler to handler the reqeusts

Author: Zhihui Chen

References:
    https://github.com/dbader/schedule
    https://github.com/mrhwick/schedule
"""
import datetime
import functools
import time
import threading
from job import Job
from logger import logger
from util import Singleton, now
from dbadapter import db


class Scheduler(object):
    """ a comprehensive job scheduler """
    __metaclass__ = Singleton

    def __init__(self):
        self.jobs = []
        self._add_gc_job()
        self.is_active = False

    def run_pending(self):
        """Run all jobs that are scheduled to run.
        Please note that it is *intended behavior that tick() does not
        run missed jobs*. For example, if you've registered a job that
        should run every minute and you only call tick() in one hour
        increments then your job won't be run 60 times in between but
        only once.
        """
        runnable_jobs = (job for job in self.jobs if job.should_run)
        for job in sorted(runnable_jobs):
            self._run_job(job)

    def run_all(self, delay_seconds=0):
        """Run all jobs regardless if they are scheduled to run or not.
        A delay of `delay` seconds is added between each job. This helps
        distribute system load generated by the jobs more evenly
        over time."""
        logger.info('Running *all* %i jobs with %is delay inbetween',
                    len(self.jobs), delay_seconds)
        for job in self.jobs:
            self._run_job(job)
            time.sleep(delay_seconds)

    def run_continuously(self, interval):
        """Continuously run, while executing pending jobs at each elapsed
        time interval.
        """
        self.run_event = threading.Event()
        self.is_active = True
        class ScheduleThread(threading.Thread):
            @classmethod
            def run(cls):
                self.thread_id = threading.currentThread().ident
                while self.is_active:
                    self.run_event.wait()
                    self.run_pending()
                    time.sleep(interval)
                logger.info('main runnging thread (%s) exits.' % self.thread_id)

        continuous_thread = ScheduleThread()
        continuous_thread.start()
        return (self.run_event, continuous_thread)

    def snapshot(self):
        """ capture the current runtime status of scheduler for debug.
        if the scheduler crashes, user can restore the scene. """
        pass

    def _add_gc_job(self):
        job = Job()
        job.name = 'gc'
        job.job_func= self.gc
        job.set_frequency(10, 'minutes')
        self._add_job(job)
        self.instantiate(job)

    def gc(self):
        """ garbage collection for the invalid jobs """
        logger.info('garbage collection starts')
        self.pause()
        self.jobs = [job for job in self.jobs if not job.need_gc()]
        self.play()
        logger.info('garbage collection stops')

    def start(self, interval=1):
        """ start the schduler """
        if self.is_active:
            logger.info('scheduler already starts, and skip it')
        else:
            self.run_continuously(interval)
            self.play()
            logger.info('scheduler start with thread_id = %s and interval = %s' % (self.thread_id, interval))

    def stop(self):
        """ stop the backend main thread,
        only available for 'run_continuously' """
        self.is_active = False
        logger.info('scheduler stop')

    def play(self):
        """ continue the backend main thread,
        only available for 'run_continuously' """
        if self.run_event and not self.run_event.is_set():
            self.run_event.set()
        logger.info('scheduler play')

    def pause(self):
        """ pause the backend main thread,
        only available for 'run_continuously' """
        if self.run_event and self.run_event.is_set():
            self.run_event.clear()
        logger.info('scheduler pause')

    def clear(self):
        """Deletes all scheduled jobs."""
        del self.jobs[:]

    def _run_job(self, job, is_parallel = True):
        ret = job.run(is_parallel)
        dic = {'status': 'Running', 'start_time': now()}
        db.update_runtime(dic, {'job_name': job.name})
        logger.info('run job: %s' % job.name)

    def add_job(self, kwargs):
        """ parse the job information,
        and add it to the running queue """
        job = None
        type_ = kwargs['file'].split('.')[-1]
        if type_ == 'job':
            from dotjob_parser import DotJobParser
            job = DotJobParser().parse(kwargs['content'])
        return self._add_job(job)

    def _add_job(self, job):
        exist_job = self._find_job(job.name)
        if exist_job and True: # TODO: flag to replace the job
            self._remove_job(exist_job)
        self.jobs.append(job)
        db.update_meta(job.job_meta())
        logger.info('add job: %s' % job.name)
        return job

    def instantiate(self, job, user=None):
        """ kickoff the job, make this job active """
        job.instantiate(callback=self.callback)
        dic = job.runtime_info()
        dic['status'] = 'Pending'
        dic['start_time'] = now()
        dic['owner'] = user if user else 'null'
        db.insert_runtime(dic)
        logger.info('instantiate job: %s' % job.name)

    def trigger_job(self, kwargs):
        """ manually trigger the job.
        only available when the target job is finished (success or fail) """
        kwargs['level'] = 1
        return self._recur_trigger(kwargs, True)

    def _recur_trigger(self, kwargs, force=False):
        """ recursively trigger the depend jobs.
        skip it when the dep job is success """
        stat_list = ['fail']
        name = kwargs['name']
        if force:
            stat_list.append('success')
        # TODO: if the job is gced, we should reload it from the db
        job = self._find_job(name)
        if job:
            stat = self.job_status(kwargs)
            if stat and stat.lower() in stat_list:
                self.instantiate(job, kwargs['user'])
                ret = 'trigger job: %s' % name
                deps = self.job_dep(kwargs)
                if len(deps) == 1:
                    for dep in deps[name]:
                        kwargs['name'] = dep
                        self._recur_trigger(kwargs)
            else:
                ret = 'job(%s) is under %s. skip it.' % (name, stat)
        else:
            ret = 'can not find the job: %s' % name
        logger.info(ret)
        return ret

    def callback(self, *args):
        """ register the callback function when the job is finished """
        stat = 'Success' if args[0] == 0 else 'Fail'
        job = args[2]
        dic = {'status': stat, 'stop_time': now()}
        db.update_runtime(dic, {'job_name': job.name})
        logger.info('finish job %s with status %s' % (job.name, stat))

    def remove_job(self, kwargs):
        """ permanently remove the job from job queue """
        job = self._find_job(kwargs['job'])
        if job:
            self._remove_job(job)

    def _remove_job(self, job):
        if job in self.jobs:
            self.jobs.remove(job)
        logger.info('remove job: %s' % job.name)

    def freeze_job(self, name):
        """ TODO: add a valid member in the Job """
        pass

    def _find_job(self, name):
        """ find the job accroding to the given name """
        for job in self.jobs:
            if job.name == name:
                return job
        return None

    def job_status(self, kwargs):
        """ show the job status.
        there are four status until now:
        Pending: job is kicked off, but waiting for some conditions
        Running: job is running
        Success: job is successfully finished
        Fail:    job is finished, but some errors happend"""
        name = kwargs['name']
        rsp = db.query_runtime(['status'],{'job_name':name})
        ret = rsp[0] if rsp and len(rsp) > 0 else 'Unknown'
        return ret

    def runtime_infos(self, kwargs):
        """ return all the runtime jobs information """
        infos = []
        for job in self.jobs:
            rsp = db.query_runtime([],{'job_name':job.name})
            if rsp:
                infos.append(rsp)
        return infos

    def job_message(self, kwargs):
        """ show the job running message.
        it will display all the conditions that the current job depends on """
        job = self._find_job(kwargs['name'])
        return job.message if job else "job cann't be found."

    def job_dep(self, kwargs):
        """ show the job dependencies """
        level = 1 if 'level' not in kwargs.keys() else kwargs['level']
        dep_tree = {}
        self._recur_dep(kwargs['name'], 0, level, dep_tree)
        print '=>', dep_tree
        return dep_tree

    def _recur_dep(self, name, level, max_level, dep_tree):
        if level >= max_level:
            return
        rsp = db.query_runtime(['full_dep'],{'job_name':name})
        if rsp and len(rsp) > 0:
            deps = rsp[0].split(';') if rsp[0] != '' else []
            dep_tree[name] = deps
            for dep in deps:
                self._recur_dep(dep, level+1, max_level, dep_tree)

    @property
    def next_run(self):
        """ datetime when the next job should run."""
        if not self.jobs:
            return None
        return min(self.jobs).next_run

    @property
    def idle_seconds(self):
        """ number of seconds until `next_run`."""
        return (self.next_run - now()).total_seconds()

